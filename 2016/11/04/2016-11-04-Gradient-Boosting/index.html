<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>梯度提升(Gradient Boosting) | MayMoon</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="什么是梯度提升(Gradient Boosting)可以认为 Gradient Boosting = Gradient Descent + Boosting在AdaBoost中，我们通过每一步添加一个弱分类器来最终得到一个由若干弱分类器线性组合成的强分类器。每一步训练弱分类器时都更关注之前误分类的样本。将误分类样本看成总模型的缺陷，则算法每一步都是通过添加一个弱分类器来试图减小总模型的缺陷。和 A">
<meta property="og:type" content="article">
<meta property="og:title" content="梯度提升(Gradient Boosting)">
<meta property="og:url" content="http://yoursite.com/2016/11/04/2016-11-04-Gradient-Boosting/index.html">
<meta property="og:site_name" content="MayMoon">
<meta property="og:description" content="什么是梯度提升(Gradient Boosting)可以认为 Gradient Boosting = Gradient Descent + Boosting在AdaBoost中，我们通过每一步添加一个弱分类器来最终得到一个由若干弱分类器线性组合成的强分类器。每一步训练弱分类器时都更关注之前误分类的样本。将误分类样本看成总模型的缺陷，则算法每一步都是通过添加一个弱分类器来试图减小总模型的缺陷。和 A">
<meta property="og:image" content="http://i.imgur.com/3tfeYa5.png">
<meta property="og:image" content="http://i.imgur.com/AeP3Cn1.png">
<meta property="og:image" content="http://i.imgur.com/UfJc50o.png">
<meta property="og:updated_time" content="2016-11-04T02:40:00.664Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="梯度提升(Gradient Boosting)">
<meta name="twitter:description" content="什么是梯度提升(Gradient Boosting)可以认为 Gradient Boosting = Gradient Descent + Boosting在AdaBoost中，我们通过每一步添加一个弱分类器来最终得到一个由若干弱分类器线性组合成的强分类器。每一步训练弱分类器时都更关注之前误分类的样本。将误分类样本看成总模型的缺陷，则算法每一步都是通过添加一个弱分类器来试图减小总模型的缺陷。和 A">
<meta name="twitter:image" content="http://i.imgur.com/3tfeYa5.png">
  
    <link rel="alternate" href="/atom.xml" title="MayMoon" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">MayMoon</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">随便写写</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="Flux RSS"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Rechercher"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-2016-11-04-Gradient-Boosting" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/11/04/2016-11-04-Gradient-Boosting/" class="article-date">
  <time datetime="2016-11-04T11:54:01.000Z" itemprop="datePublished">2016-11-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      梯度提升(Gradient Boosting)
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="什么是梯度提升-Gradient-Boosting"><a href="#什么是梯度提升-Gradient-Boosting" class="headerlink" title="什么是梯度提升(Gradient Boosting)"></a>什么是梯度提升(Gradient Boosting)</h2><p>可以认为 <strong>Gradient Boosting = Gradient Descent + Boosting</strong><br>在AdaBoost中，我们通过每一步添加一个弱分类器来最终得到一个由若干弱分类器线性组合成的强分类器。每一步训练弱分类器时都更关注之前<strong>误分类的样本</strong>。将<strong>误分类样本</strong>看成总模型的缺陷，则算法每一步都是通过添加一个弱分类器来试图减小总模型的缺陷。<br>和 AdaBoost 一样，Gradient Boosting 也是一个步进算法，每一步添加一个弱分类器。不同的是，在 Gradient Boosting中，用来刻画缺陷的是<strong>梯度</strong>，通过梯度下降，来逐渐提高总模型的预测能力。</p>
<a id="more"></a>
<h3 id="一个小例子"><a href="#一个小例子" class="headerlink" title="一个小例子"></a>一个小例子</h3><p>给定了一组数据点 $(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)$，要求给出一个模型 $F(x)$ 使得平方误差最小。<br>假设你已经得到了一个模型：有 $F(x_1)=0.8$，$y_1=0.9$，$F(x_2)=1.4$，$y_1=1.3$ …… 现在想改进这个模型，要怎么做？</p>
<p>最简单的办法就是添加一个模型 $h$，即新的预测值由 $F(x)+h(x)$ 给出。<br>我们希望改进后满足$$\begin{array}{c}<br>F(x_1)+h(x_1)=y_1 \\<br>F(x_2)+h(x_2)=y_2 \\<br>\cdots \\<br>F(x_n)+h(x_n)=y_n<br>\end{array}$$<br>等价地，我们希望$$\begin{array}{c}<br>h(x_1)=y_1-F(x_1) \\<br>h(x_2)=y_2-F(x_2) \\<br>\cdots \\<br>h(x_n)=y_n-F(x_n) \\<br>\end{array}$$<br>所以我们可以训练出来一个模型 $h$ 来拟合数据 $(x_1,y_1-F(x_1)),(x_2,y_2-F(x_2)),\cdots,(x_n,y_n-F(x_n))$。如果添加 $h$ 后，我们对总模型仍不满意，可以继续添加新的模型。</p>
<h3 id="与梯度下降的关系"><a href="#与梯度下降的关系" class="headerlink" title="与梯度下降的关系"></a>与梯度下降的关系</h3><p>上述的内容看起来与梯度下降没有关系，实则不然。回顾梯度下降的含义：通过向负梯度方向移动来最小化函数，即$$\theta_i:=\theta_i-\rho\cfrac{\partial J}{\partial \theta_i}$$<br>在上面的例子中，损失函数是 $L(y,F(x))=(y-F(x))^2/2$，我们想要通过调整 $F(x_1),\cdots,F(x_n)$ 来最小化 $J\left(\sum_{i=1}^n L(y_i,F(x_i))\right)$。则<br>$$\cfrac{\partial J}{\partial F(x_i)}=\cfrac{\partial \sum_{i=1}^n L(y_i,F(x_i))}{\partial F(x_i)}=\cfrac{\partial L(y_i,F(x_i))}{\partial F(x_i)}=F(x_i)-y_i$$<br>所以<br>$$\begin{array}{c,l}<br>F(x_i)&amp;:=&amp;F(x_i)+h(x_i) \\<br>&amp;:=&amp;F(x_i)+y_i-F(x_i) \\<br>&amp;:=&amp;F(x_i)-1\cfrac{\partial J}{\partial F(x_i)}<br>\end{array}$$</p>
<h2 id="梯度提升回归（Gradient-Boosting-for-Regression）"><a href="#梯度提升回归（Gradient-Boosting-for-Regression）" class="headerlink" title="梯度提升回归（Gradient Boosting for Regression）"></a>梯度提升回归（Gradient Boosting for Regression）</h2><p>总结一下上面的算法。负梯度为<br>$$-g(x_i)=-\cfrac{\partial L(y_i,F(x_i))}{\partial F(x_i)}=y_i-F(x_i)$$<br>从一个最初的模型开始，比方说，$F(x_i)=\cfrac{\sum_{i=1}^n y_i}{n}$<br>重复下面过程直到收敛：</p>
<ul>
<li>计算负梯度 $-g(x_i)$</li>
<li>对负梯度 $-g(x_i)$ 拟合一个模型 $h$（比如一个回归树）</li>
<li>$F:=F+\rho h$，其中 $\rho=1$</li>
</ul>
<p>这样描述算法有助于我们通过改变损失函数来推导出其他类似的算法。</p>
<h3 id="其他损失函数"><a href="#其他损失函数" class="headerlink" title="其他损失函数"></a>其他损失函数</h3><p>在上面，我们使用的是平方误差作为损失函数</p>
<ul>
<li><strong>优点</strong>：在数学上易于处理</li>
<li><strong>缺点</strong>：对离群值很敏感（离群值会被很严重地惩罚，因为误差被平方放大了）</li>
</ul>
<p>举个例子：</p>
<table>
<thead>
<tr>
<th style="text-align:center">$y_i$</th>
<th style="text-align:center">0.5</th>
<th style="text-align:center">1.2</th>
<th style="text-align:center">2</th>
<th style="text-align:center">5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$F(x_i)$</td>
<td style="text-align:center">0.6</td>
<td style="text-align:center">1.4</td>
<td style="text-align:center">1.5</td>
<td style="text-align:center">1.7</td>
</tr>
<tr>
<td style="text-align:center">$L=(y-F)^2/2 $</td>
<td style="text-align:center">0.005</td>
<td style="text-align:center">0.02</td>
<td style="text-align:center">0.125</td>
<td style="text-align:center">5.445</td>
</tr>
</tbody>
</table>
<p>这样的后果是给予了离群值过多的关注，过于努力地想把离群值纳入到模型中，从而使得模型地整体性能变差。</p>
<p>我们可以考虑其他的损失函数，比如下面这两个：</p>
<ol>
<li>绝对值损失（不易受离群值影响）<br>$$L(y,F)=|y-F|$$</li>
<li>Huber损失（不易受离群值影响）<br>$$L(y,F)=\begin{cases}<br>\frac{1}{2}(y-F)^2&amp;|y-F|\le \delta \\<br>\delta(|y-F|-\delta/2)&amp;|y-F|&gt;\delta<br>\end{cases}$$</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align:center">$y_i$</th>
<th style="text-align:center">0.5</th>
<th style="text-align:center">1.2</th>
<th style="text-align:center">2</th>
<th style="text-align:center">5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$F(x_i)$</td>
<td style="text-align:center">0.6</td>
<td style="text-align:center">1.4</td>
<td style="text-align:center">1.5</td>
<td style="text-align:center">1.7</td>
</tr>
<tr>
<td style="text-align:center">Square Loss</td>
<td style="text-align:center">0.005</td>
<td style="text-align:center">0.02</td>
<td style="text-align:center">0.125</td>
<td style="text-align:center">5.445</td>
</tr>
<tr>
<td style="text-align:center">Absolute Loss</td>
<td style="text-align:center">0.1</td>
<td style="text-align:center">0.2</td>
<td style="text-align:center">0.5</td>
<td style="text-align:center">3.3</td>
</tr>
<tr>
<td style="text-align:center">Huber Loss<br>($\delta=0.5$)</td>
<td style="text-align:center">0.005</td>
<td style="text-align:center">0.02</td>
<td style="text-align:center">0.125</td>
<td style="text-align:center">1.525</td>
</tr>
</tbody>
</table>
<p>更多的损失函数选择可参见参考资料2</p>
<h3 id="Regression-with-Absolute-Loss"><a href="#Regression-with-Absolute-Loss" class="headerlink" title="Regression with Absolute Loss"></a>Regression with Absolute Loss</h3><p>负梯度为<br>$$-g(x_i)=-\cfrac{\partial L(y_i,F(x_i))}{\partial F(x_i)}=\text{sign}(y_i-F(x_i))$$<br>从一个最初的模型开始，比方说，$F(x_i)=\cfrac{\sum_{i=1}^n y_i}{n}$<br>重复下面过程直到收敛：</p>
<ul>
<li>计算负梯度 $-g(x_i)$</li>
<li>对负梯度 $-g(x_i)$ 拟合一个模型 $h$（比如一个回归树）</li>
<li>$F:=F+\rho h$，其中 $\rho=1$</li>
</ul>
<h3 id="Regression-with-Huber-Loss"><a href="#Regression-with-Huber-Loss" class="headerlink" title="Regression with Huber Loss"></a>Regression with Huber Loss</h3><p>负梯度为<br>$$-g(x_i)=-\cfrac{\partial L(y_i,F(x_i))}{\partial F(x_i)}=\begin{cases}<br>y_i-F(x_i)&amp;|y_i-F(x_i)|\le \delta \\<br>\delta \text{sign}(y_i-F(x_i))&amp;|y_i-F(x_i)|&gt; \delta<br>\end{cases}$$<br>从一个最初的模型开始，比方说，$F(x_i)=\cfrac{\sum_{i=1}^n y_i}{n}$<br>重复下面过程直到收敛：</p>
<ul>
<li>计算负梯度 $-g(x_i)$</li>
<li>对负梯度 $-g(x_i)$ 拟合一个模型 $h$（比如一个回归树）</li>
<li>$F:=F+\rho h$，其中 $\rho=1$</li>
</ul>
<h2 id="梯度提升分类（Gradient-Boosting-Classification）"><a href="#梯度提升分类（Gradient-Boosting-Classification）" class="headerlink" title="梯度提升分类（Gradient Boosting Classification）"></a>梯度提升分类（Gradient Boosting Classification）</h2><p>我们以一个具体的例子为场景：识别手写的大写字母。这是一个多分类问题。和回归不同的是，我们现在有26个模型函数 $F_A,F_B,F_C,\cdots,F_Z$，其中每个模型用来计算出样本属于该类别的分数，比如 $F_A(x)$ 表示这个字母是A的分数（分数越高，越可能属于这一类），然后将分数转换成概率：<br>$$\begin{array}{c}<br>P_A(x)=\cfrac{e^{F_A(x)}}{\sum_{c=A}^Ze^{F_c(x)}} \\<br>P_B(x)=\cfrac{e^{F_A(x)}}{\sum_{c=A}^Ze^{F_c(x)}} \\<br>\cdots \\<br>P_Z(x)=\cfrac{e^{F_A(x)}}{\sum_{c=A}^Ze^{F_c(x)}} \\<br>\end{array}$$<br>最终的预测结果即是概率最高的那一类</p>
<p>以 $y_5=G$ 为例，真实的概率分布为<br><img src="http://i.imgur.com/3tfeYa5.png" alt=""><br>我们逐渐调整 $F_A(x),\cdots,F_Z(x)$，使得预测的概率分布和真实的概率分布之间的差别逐渐缩小<br><img src="http://i.imgur.com/AeP3Cn1.png" alt=""><br>用KL散度来刻画两个分布之间的差别，我们的目标就是最小化总的散度。</p>
<h3 id="分类和回归的区别"><a href="#分类和回归的区别" class="headerlink" title="分类和回归的区别"></a>分类和回归的区别</h3><ol>
<li>$F_A,F_B,\cdots,F_Z$ vs $F$</li>
<li>优化的参数是一个矩阵 vs 优化的目标是一个向量<br>$$\begin{matrix}<br>F_A(x_1)&amp;F_B(x_1)&amp;\cdots&amp;F_Z(x_1) \\<br>F_A(x_2)&amp;F_B(x_2)&amp;\cdots&amp;F_Z(x_2) \\<br>\cdots&amp;\cdots&amp;\cdots&amp;\cdots \\<br>F_A(x_n)&amp;F_B(x_n)&amp;\cdots&amp;F_Z(x_n)<br>\end{matrix}$$</li>
<li>计算的是一个梯度矩阵 vs 计算的是一个梯度向量</li>
</ol>
<h3 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h3><p>从一组初始模型开始：$F_A,F_B,\cdots,F_Z$</p>
<p>重复下列步骤直到收敛：</p>
<ul>
<li>对类A计算负梯度：$-g_A(x_i)=Y_A(x_i)-P_A(x_i)$</li>
<li>对类B计算负梯度：$-g_B(x_i)=Y_B(x_i)-P_B(x_i)$<br>$\cdots$</li>
<li>对类Z计算负梯度：$-g_Z(x_i)=Y_Z(x_i)-P_Z(x_i)$</li>
<li>训练一个模型（回归树）$h_A$ 来拟合负梯度 $-g_A(x_i)$</li>
<li>训练一个模型（回归树）$h_B$ 来拟合负梯度 $-g_B(x_i)$<br>$\cdots$</li>
<li>训练一个模型（回归树）$h_Z$ 来拟合负梯度 $-g_Z(x_i)$</li>
<li>$F_A:=F_A+\rho_Ah_A$</li>
<li>$F_B:=F_B+\rho_Bh_B$<br>$\cdots$</li>
<li>$F_Z:=F_Z+\rho_Zh_Z$</li>
</ul>
<h2 id="选择步长"><a href="#选择步长" class="headerlink" title="选择步长"></a>选择步长</h2><p>在预测过程中，我们每次添加一个弱分类器，但过多的分类器会导致过拟合，所以要对分类器的个数 $M$ 加以限制。另一方面，有人发现使用收缩技术可以很好地防止过拟合，一个简单的收缩技术就是在每一步中加入<strong>步长</strong>参数 $v$，即<br>$$F_m(\mathbf{x})=F_{m-1}(\mathbf{x})+v\cdot\rho_mh(\mathbf{x}),\quad 0&lt;v\le 1$$<br>我们现在有两个参数可以来约束模型：$M$ 和 $v$。它们之间互相影响，理想情况是从两个参数的联合分布中选出最合适的取值。不过，增大 $M$ 也会同时增大计算量。下图是在某个数据集上做的测试，四条曲线分别代表 $v\in\{1.0,0.25,0.125,0.06\}$。</p>
<p><img src="http://i.imgur.com/UfJc50o.png" alt=""></p>
<p>可以看出：</p>
<ol>
<li>当 $v$ 比较大时，可以看到很明显的过拟合：$M$ 增大到一定程度时模型达到最优解，再增大 $M$，模型会变差。</li>
<li>$v$ 值越小，收缩得越慢</li>
<li>$v$ 值越小，过拟合越不明显</li>
<li>当 $v$ 值比较小得时候，模型的最优解几乎不再变化</li>
</ol>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf" target="_blank" rel="external">Gradient Boosting</a></li>
<li><a href="https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451" target="_blank" rel="external">Greedy Function Approximation: A Gradient Boosting Machine</a></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/11/04/2016-11-04-Gradient-Boosting/" data-id="civ37veb3002jogkr01f3mcvq" class="article-share-link">Partager</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/gbdt/">gbdt</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2016/10/28/2016-10-28-lars-lasso-stagewise/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Ancien</strong>
      <div class="article-nav-title">LARS与Lasso和Forward Stagewise</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Catégories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/CS229-notes/">CS229 notes</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/LeetCode笔记/">LeetCode笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning-Course-notes/">Machine Learning Course notes</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/PRML-notes/">PRML notes</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Under-Construction/">Under Construction</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/《Algorithms》-notes/">《Algorithms》 notes</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/优化算法/">优化算法</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/具体数学笔记/">具体数学笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/杂记/">杂记</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Mot-clés</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/a-algorithm/">a* algorithm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/adaboost/">adaboost</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/add-digits/">add digits</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/algorithm/">algorithm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/arithmetic-expression/">arithmetic expression</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bag/">bag</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bagging/">bagging</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/batch-gradient-decent/">batch gradient decent</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/beam-search/">beam search</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/best-first-search/">best-first search</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bfs/">bfs</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bias/">bias</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/binary-search/">binary search</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/binary-search-tree/">binary search tree</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/curve-fitting/">curve fitting</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/dag/">dag</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/data-structure/">data structure</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/dfs/">dfs</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/directed-graph/">directed graph</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/exponential-family-distribution/">exponential family distribution</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/forward-stagewise/">forward stagewise</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/gbdt/">gbdt</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/general-linear-model/">general linear model</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hanoi-tower/">hanoi tower</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hash/">hash</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/heap-sort/">heap sort</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/heuristic-algorithm/">heuristic algorithm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/">hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/insertion-sort/">insertion sort</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/josephus-problem/">josephus problem</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/knight-tour/">knight tour</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kosaraju-s-algorithm/">kosaraju's algorithm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kruskal-s-algorithm/">kruskal's algorithm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/lars/">lars</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/lasso/">lasso</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/leetcode/">leetcode</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linear-regression/">linear regression</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/lingo/">lingo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/logistic-regression/">logistic regression</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/lwr/">lwr</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/machine-learning/">machine learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mathjax/">mathjax</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mst/">mst</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/neural-network/">neural network</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nim-game/">nim game</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/physics/">physics</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/prim-s-algorithm/">prim's algorithm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/priority-queue/">priority queue</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/prml/">prml</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/queue/">queue</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/random-forest/">random forest</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/selection-sort/">selection sort</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell-sort/">shell sort</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/single-number/">single number</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/stack/">stack</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/stochastic-gradient-decent/">stochastic gradient decent</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/support-vector-machine/">support vector machine</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/symbol-table/">symbol table</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/topological-sort/">topological sort</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/undirected-graph/">undirected graph</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/variance/">variance</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/递归问题/">递归问题</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/集成学习/">集成学习</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Nuage de mot-clés</h3>
    <div class="widget tagcloud">
      <a href="/tags/a-algorithm/" style="font-size: 10px;">a* algorithm</a> <a href="/tags/adaboost/" style="font-size: 10px;">adaboost</a> <a href="/tags/add-digits/" style="font-size: 10px;">add digits</a> <a href="/tags/algorithm/" style="font-size: 20px;">algorithm</a> <a href="/tags/arithmetic-expression/" style="font-size: 10px;">arithmetic expression</a> <a href="/tags/bag/" style="font-size: 10px;">bag</a> <a href="/tags/bagging/" style="font-size: 10px;">bagging</a> <a href="/tags/batch-gradient-decent/" style="font-size: 10px;">batch gradient decent</a> <a href="/tags/beam-search/" style="font-size: 10px;">beam search</a> <a href="/tags/best-first-search/" style="font-size: 10px;">best-first search</a> <a href="/tags/bfs/" style="font-size: 10px;">bfs</a> <a href="/tags/bias/" style="font-size: 10px;">bias</a> <a href="/tags/binary-search/" style="font-size: 10px;">binary search</a> <a href="/tags/binary-search-tree/" style="font-size: 10px;">binary search tree</a> <a href="/tags/curve-fitting/" style="font-size: 10px;">curve fitting</a> <a href="/tags/dag/" style="font-size: 10px;">dag</a> <a href="/tags/data-structure/" style="font-size: 10px;">data structure</a> <a href="/tags/dfs/" style="font-size: 13.33px;">dfs</a> <a href="/tags/directed-graph/" style="font-size: 10px;">directed graph</a> <a href="/tags/exponential-family-distribution/" style="font-size: 10px;">exponential family distribution</a> <a href="/tags/forward-stagewise/" style="font-size: 10px;">forward stagewise</a> <a href="/tags/gbdt/" style="font-size: 10px;">gbdt</a> <a href="/tags/general-linear-model/" style="font-size: 10px;">general linear model</a> <a href="/tags/hanoi-tower/" style="font-size: 10px;">hanoi tower</a> <a href="/tags/hash/" style="font-size: 10px;">hash</a> <a href="/tags/heap-sort/" style="font-size: 10px;">heap sort</a> <a href="/tags/heuristic-algorithm/" style="font-size: 10px;">heuristic algorithm</a> <a href="/tags/hexo/" style="font-size: 13.33px;">hexo</a> <a href="/tags/insertion-sort/" style="font-size: 10px;">insertion sort</a> <a href="/tags/josephus-problem/" style="font-size: 10px;">josephus problem</a> <a href="/tags/knight-tour/" style="font-size: 10px;">knight tour</a> <a href="/tags/kosaraju-s-algorithm/" style="font-size: 10px;">kosaraju's algorithm</a> <a href="/tags/kruskal-s-algorithm/" style="font-size: 10px;">kruskal's algorithm</a> <a href="/tags/lars/" style="font-size: 13.33px;">lars</a> <a href="/tags/lasso/" style="font-size: 13.33px;">lasso</a> <a href="/tags/leetcode/" style="font-size: 10px;">leetcode</a> <a href="/tags/linear-regression/" style="font-size: 10px;">linear regression</a> <a href="/tags/lingo/" style="font-size: 10px;">lingo</a> <a href="/tags/logistic-regression/" style="font-size: 10px;">logistic regression</a> <a href="/tags/lwr/" style="font-size: 10px;">lwr</a> <a href="/tags/machine-learning/" style="font-size: 16.67px;">machine learning</a> <a href="/tags/mathjax/" style="font-size: 10px;">mathjax</a> <a href="/tags/mst/" style="font-size: 10px;">mst</a> <a href="/tags/neural-network/" style="font-size: 10px;">neural network</a> <a href="/tags/nim-game/" style="font-size: 10px;">nim game</a> <a href="/tags/physics/" style="font-size: 10px;">physics</a> <a href="/tags/prim-s-algorithm/" style="font-size: 10px;">prim's algorithm</a> <a href="/tags/priority-queue/" style="font-size: 10px;">priority queue</a> <a href="/tags/prml/" style="font-size: 10px;">prml</a> <a href="/tags/queue/" style="font-size: 10px;">queue</a> <a href="/tags/random-forest/" style="font-size: 10px;">random forest</a> <a href="/tags/selection-sort/" style="font-size: 10px;">selection sort</a> <a href="/tags/shell-sort/" style="font-size: 10px;">shell sort</a> <a href="/tags/single-number/" style="font-size: 10px;">single number</a> <a href="/tags/stack/" style="font-size: 10px;">stack</a> <a href="/tags/stochastic-gradient-decent/" style="font-size: 10px;">stochastic gradient decent</a> <a href="/tags/support-vector-machine/" style="font-size: 10px;">support vector machine</a> <a href="/tags/symbol-table/" style="font-size: 13.33px;">symbol table</a> <a href="/tags/topological-sort/" style="font-size: 10px;">topological sort</a> <a href="/tags/undirected-graph/" style="font-size: 10px;">undirected graph</a> <a href="/tags/variance/" style="font-size: 10px;">variance</a> <a href="/tags/递归问题/" style="font-size: 10px;">递归问题</a> <a href="/tags/集成学习/" style="font-size: 10px;">集成学习</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">June 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">April 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2000/07/">July 2000</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2000/05/">May 2000</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Articles récents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/11/04/2016-11-04-Gradient-Boosting/">梯度提升(Gradient Boosting)</a>
          </li>
        
          <li>
            <a href="/2016/10/28/2016-10-28-lars-lasso-stagewise/">LARS与Lasso和Forward Stagewise</a>
          </li>
        
          <li>
            <a href="/2016/10/26/2016-10-26-Least-Angle-Regression/">最小角回归(Least Angle Regression)</a>
          </li>
        
          <li>
            <a href="/2016/10/17/2016-10-17-集成学习算法/">集成学习算法(Ensemble Learning)</a>
          </li>
        
          <li>
            <a href="/2016/10/14/2016-10-14-General-Linear-Model/">一般线性模型(General Linear Model)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 Wang Kx<br>
      Propulsé by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>