<!doctype html>



  


<html class="theme-next muse use-motion">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"/>




  <link href="//fonts.googleapis.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">



<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=0.5.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="gbdt," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=0.5.0" />






<meta name="description" content="什么是梯度提升(Gradient Boosting)可以认为 Gradient Boosting = Gradient Descent + Boosting在AdaBoost中，我们通过每一步添加一个弱分类器来最终得到一个由若干弱分类器线性组合成的强分类器。每一步训练弱分类器时都更关注之前误分类的样本。将误分类样本看成总模型的缺陷，则算法每一步都是通过添加一个弱分类器来试图减小总模型的缺陷。和 A">
<meta property="og:type" content="article">
<meta property="og:title" content="梯度提升(Gradient Boosting)">
<meta property="og:url" content="http://yoursite.com/2016/11/04/2016-11-04-Gradient-Boosting/index.html">
<meta property="og:site_name" content="MayMoon">
<meta property="og:description" content="什么是梯度提升(Gradient Boosting)可以认为 Gradient Boosting = Gradient Descent + Boosting在AdaBoost中，我们通过每一步添加一个弱分类器来最终得到一个由若干弱分类器线性组合成的强分类器。每一步训练弱分类器时都更关注之前误分类的样本。将误分类样本看成总模型的缺陷，则算法每一步都是通过添加一个弱分类器来试图减小总模型的缺陷。和 A">
<meta property="og:image" content="http://i.imgur.com/3tfeYa5.png">
<meta property="og:image" content="http://i.imgur.com/AeP3Cn1.png">
<meta property="og:image" content="http://i.imgur.com/UfJc50o.png">
<meta property="og:updated_time" content="2016-11-18T06:07:57.556Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="梯度提升(Gradient Boosting)">
<meta name="twitter:description" content="什么是梯度提升(Gradient Boosting)可以认为 Gradient Boosting = Gradient Descent + Boosting在AdaBoost中，我们通过每一步添加一个弱分类器来最终得到一个由若干弱分类器线性组合成的强分类器。每一步训练弱分类器时都更关注之前误分类的样本。将误分类样本看成总模型的缺陷，则算法每一步都是通过添加一个弱分类器来试图减小总模型的缺陷。和 A">
<meta name="twitter:image" content="http://i.imgur.com/3tfeYa5.png">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"hide"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>

  <title> 梯度提升(Gradient Boosting) | MayMoon </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta custom-logo">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">MayMoon</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">随便写写</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-home fa-fw"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-th fa-fw"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-archive fa-fw"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-tags fa-fw"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                梯度提升(Gradient Boosting)
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-11-04T19:54:01+08:00" content="2016-11-04">
              2016-11-04
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="什么是梯度提升-Gradient-Boosting"><a href="#什么是梯度提升-Gradient-Boosting" class="headerlink" title="什么是梯度提升(Gradient Boosting)"></a>什么是梯度提升(Gradient Boosting)</h2><p>可以认为 <strong>Gradient Boosting = Gradient Descent + Boosting</strong><br>在AdaBoost中，我们通过每一步添加一个弱分类器来最终得到一个由若干弱分类器线性组合成的强分类器。每一步训练弱分类器时都更关注之前<strong>误分类的样本</strong>。将<strong>误分类样本</strong>看成总模型的缺陷，则算法每一步都是通过添加一个弱分类器来试图减小总模型的缺陷。<br>和 AdaBoost 一样，Gradient Boosting 也是一个步进算法，每一步添加一个弱分类器。不同的是，在 Gradient Boosting中，用来刻画缺陷的是<strong>梯度</strong>，通过梯度下降，来逐渐提高总模型的预测能力。</p>
<a id="more"></a>
<h3 id="一个小例子"><a href="#一个小例子" class="headerlink" title="一个小例子"></a>一个小例子</h3><p>给定了一组数据点 $(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)$，要求给出一个模型 $F(x)$ 使得平方误差最小。<br>假设你已经得到了一个模型：有 $F(x_1)=0.8$，$y_1=0.9$，$F(x_2)=1.4$，$y_2=1.3$ …… 现在想改进这个模型，要怎么做？</p>
<p>最简单的办法就是添加一个模型 $h$，即新的预测值由 $F(x)+h(x)$ 给出。<br>我们希望改进后满足$$\begin{array}{c}<br>F(x_1)+h(x_1)=y_1 \\<br>F(x_2)+h(x_2)=y_2 \\<br>\cdots \\<br>F(x_n)+h(x_n)=y_n<br>\end{array}$$<br>等价地，我们希望$$\begin{array}{c}<br>h(x_1)=y_1-F(x_1) \\<br>h(x_2)=y_2-F(x_2) \\<br>\cdots \\<br>h(x_n)=y_n-F(x_n) \\<br>\end{array}$$<br>所以我们可以训练出来一个模型 $h$ 来拟合数据 $(x_1,y_1-F(x_1)),(x_2,y_2-F(x_2)),\cdots,(x_n,y_n-F(x_n))$。如果添加 $h$ 后，我们对总模型仍不满意，可以继续添加新的模型。</p>
<h3 id="与梯度下降的关系"><a href="#与梯度下降的关系" class="headerlink" title="与梯度下降的关系"></a>与梯度下降的关系</h3><p>上述的内容看起来与梯度下降没有关系，实则不然。回顾梯度下降的含义：通过向负梯度方向移动来最小化函数，即$$\theta_i:=\theta_i-\rho\cfrac{\partial J}{\partial \theta_i}$$<br>在上面的例子中，损失函数是 $L(y,F(x))=(y-F(x))^2/2$，我们想要通过调整 $F(x_1),\cdots,F(x_n)$ 来最小化 $J\left(\sum_{i=1}^n L(y_i,F(x_i))\right)$。则<br>$$\cfrac{\partial J}{\partial F(x_i)}=\cfrac{\partial \sum_{i=1}^n L(y_i,F(x_i))}{\partial F(x_i)}=\cfrac{\partial L(y_i,F(x_i))}{\partial F(x_i)}=F(x_i)-y_i$$<br>所以<br>$$\begin{array}{c,l}<br>F(x_i)&amp;:=&amp;F(x_i)+h(x_i) \\<br>&amp;:=&amp;F(x_i)+y_i-F(x_i) \\<br>&amp;:=&amp;F(x_i)-1\cfrac{\partial J}{\partial F(x_i)}<br>\end{array}$$</p>
<h2 id="梯度提升回归（Gradient-Boosting-for-Regression）"><a href="#梯度提升回归（Gradient-Boosting-for-Regression）" class="headerlink" title="梯度提升回归（Gradient Boosting for Regression）"></a>梯度提升回归（Gradient Boosting for Regression）</h2><p>总结一下上面的算法。负梯度为<br>$$-g(x_i)=-\cfrac{\partial L(y_i,F(x_i))}{\partial F(x_i)}=y_i-F(x_i)$$<br>从一个最初的模型开始，比方说，$F(x_i)=\cfrac{\sum_{i=1}^n y_i}{n}$<br>重复下面过程直到收敛：</p>
<ul>
<li>计算负梯度 $-g(x_i)$</li>
<li>对负梯度 $-g(x_i)$ 拟合一个模型 $h$（比如一个回归树）</li>
<li>$F:=F+\rho h$，其中 $\rho=1$</li>
</ul>
<p>这样描述算法有助于我们通过改变损失函数来推导出其他类似的算法。</p>
<h3 id="其他损失函数"><a href="#其他损失函数" class="headerlink" title="其他损失函数"></a>其他损失函数</h3><p>在上面，我们使用的是平方误差作为损失函数</p>
<ul>
<li><strong>优点</strong>：在数学上易于处理</li>
<li><strong>缺点</strong>：对离群值很敏感（离群值会被很严重地惩罚，因为误差被平方放大了）</li>
</ul>
<p>举个例子：</p>
<table>
<thead>
<tr>
<th style="text-align:center">$y_i$</th>
<th style="text-align:center">0.5</th>
<th style="text-align:center">1.2</th>
<th style="text-align:center">2</th>
<th style="text-align:center">5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$F(x_i)$</td>
<td style="text-align:center">0.6</td>
<td style="text-align:center">1.4</td>
<td style="text-align:center">1.5</td>
<td style="text-align:center">1.7</td>
</tr>
<tr>
<td style="text-align:center">$L=(y-F)^2/2 $</td>
<td style="text-align:center">0.005</td>
<td style="text-align:center">0.02</td>
<td style="text-align:center">0.125</td>
<td style="text-align:center">5.445</td>
</tr>
</tbody>
</table>
<p>这样的后果是给予了离群值过多的关注，过于努力地想把离群值纳入到模型中，从而使得模型地整体性能变差。</p>
<p>我们可以考虑其他的损失函数，比如下面这两个：</p>
<ol>
<li>绝对值损失（不易受离群值影响）<br>$$L(y,F)=|y-F|$$</li>
<li>Huber损失（不易受离群值影响）<br>$$L(y,F)=\begin{cases}<br>\frac{1}{2}(y-F)^2&amp;|y-F|\le \delta \\<br>\delta(|y-F|-\delta/2)&amp;|y-F|&gt;\delta<br>\end{cases}$$</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align:center">$y_i$</th>
<th style="text-align:center">0.5</th>
<th style="text-align:center">1.2</th>
<th style="text-align:center">2</th>
<th style="text-align:center">5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$F(x_i)$</td>
<td style="text-align:center">0.6</td>
<td style="text-align:center">1.4</td>
<td style="text-align:center">1.5</td>
<td style="text-align:center">1.7</td>
</tr>
<tr>
<td style="text-align:center">Square Loss</td>
<td style="text-align:center">0.005</td>
<td style="text-align:center">0.02</td>
<td style="text-align:center">0.125</td>
<td style="text-align:center">5.445</td>
</tr>
<tr>
<td style="text-align:center">Absolute Loss</td>
<td style="text-align:center">0.1</td>
<td style="text-align:center">0.2</td>
<td style="text-align:center">0.5</td>
<td style="text-align:center">3.3</td>
</tr>
<tr>
<td style="text-align:center">Huber Loss<br>($\delta=0.5$)</td>
<td style="text-align:center">0.005</td>
<td style="text-align:center">0.02</td>
<td style="text-align:center">0.125</td>
<td style="text-align:center">1.525</td>
</tr>
</tbody>
</table>
<p>更多的损失函数选择可参见参考资料2</p>
<h3 id="Regression-with-Absolute-Loss"><a href="#Regression-with-Absolute-Loss" class="headerlink" title="Regression with Absolute Loss"></a>Regression with Absolute Loss</h3><p>负梯度为<br>$$-g(x_i)=-\cfrac{\partial L(y_i,F(x_i))}{\partial F(x_i)}=\text{sign}(y_i-F(x_i))$$<br>从一个最初的模型开始，比方说，$F(x_i)=\cfrac{\sum_{i=1}^n y_i}{n}$<br>重复下面过程直到收敛：</p>
<ul>
<li>计算负梯度 $-g(x_i)$</li>
<li>对负梯度 $-g(x_i)$ 拟合一个模型 $h$（比如一个回归树）</li>
<li>$F:=F+\rho h$，其中 $\rho=1$</li>
</ul>
<h3 id="Regression-with-Huber-Loss"><a href="#Regression-with-Huber-Loss" class="headerlink" title="Regression with Huber Loss"></a>Regression with Huber Loss</h3><p>负梯度为<br>$$-g(x_i)=-\cfrac{\partial L(y_i,F(x_i))}{\partial F(x_i)}=\begin{cases}<br>y_i-F(x_i)&amp;|y_i-F(x_i)|\le \delta \\<br>\delta \text{sign}(y_i-F(x_i))&amp;|y_i-F(x_i)|&gt; \delta<br>\end{cases}$$<br>从一个最初的模型开始，比方说，$F(x_i)=\cfrac{\sum_{i=1}^n y_i}{n}$<br>重复下面过程直到收敛：</p>
<ul>
<li>计算负梯度 $-g(x_i)$</li>
<li>对负梯度 $-g(x_i)$ 拟合一个模型 $h$（比如一个回归树）</li>
<li>$F:=F+\rho h$，其中 $\rho=1$</li>
</ul>
<h2 id="梯度提升分类（Gradient-Boosting-Classification）"><a href="#梯度提升分类（Gradient-Boosting-Classification）" class="headerlink" title="梯度提升分类（Gradient Boosting Classification）"></a>梯度提升分类（Gradient Boosting Classification）</h2><p>我们以一个具体的例子为场景：识别手写的大写字母。这是一个多分类问题。和回归不同的是，我们现在有26个模型函数 $F_A,F_B,F_C,\cdots,F_Z$，其中每个模型用来计算出样本属于该类别的分数，比如 $F_A(x)$ 表示这个字母是A的分数（分数越高，越可能属于这一类），然后将分数转换成概率：<br>$$\begin{array}{c}<br>P_A(x)=\cfrac{e^{F_A(x)}}{\sum_{c=A}^Ze^{F_c(x)}} \\<br>P_B(x)=\cfrac{e^{F_A(x)}}{\sum_{c=A}^Ze^{F_c(x)}} \\<br>\cdots \\<br>P_Z(x)=\cfrac{e^{F_A(x)}}{\sum_{c=A}^Ze^{F_c(x)}} \\<br>\end{array}$$<br>最终的预测结果即是概率最高的那一类</p>
<p>以 $y_5=G$ 为例，真实的概率分布为<br><img src="http://i.imgur.com/3tfeYa5.png" alt=""><br>我们逐渐调整 $F_A(x),\cdots,F_Z(x)$，使得预测的概率分布和真实的概率分布之间的差别逐渐缩小<br><img src="http://i.imgur.com/AeP3Cn1.png" alt=""><br>用KL散度来刻画两个分布之间的差别，我们的目标就是最小化总的散度。</p>
<h3 id="分类和回归的区别"><a href="#分类和回归的区别" class="headerlink" title="分类和回归的区别"></a>分类和回归的区别</h3><ol>
<li>$F_A,F_B,\cdots,F_Z$ vs $F$</li>
<li>优化的参数是一个矩阵 vs 优化的目标是一个向量<br>$$\begin{matrix}<br>F_A(x_1)&amp;F_B(x_1)&amp;\cdots&amp;F_Z(x_1) \\<br>F_A(x_2)&amp;F_B(x_2)&amp;\cdots&amp;F_Z(x_2) \\<br>\cdots&amp;\cdots&amp;\cdots&amp;\cdots \\<br>F_A(x_n)&amp;F_B(x_n)&amp;\cdots&amp;F_Z(x_n)<br>\end{matrix}$$</li>
<li>计算的是一个梯度矩阵 vs 计算的是一个梯度向量</li>
</ol>
<h3 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h3><p>从一组初始模型开始：$F_A,F_B,\cdots,F_Z$</p>
<p>重复下列步骤直到收敛：</p>
<ul>
<li>对类A计算负梯度：$-g_A(x_i)=Y_A(x_i)-P_A(x_i)$</li>
<li>对类B计算负梯度：$-g_B(x_i)=Y_B(x_i)-P_B(x_i)$<br>$\cdots$</li>
<li>对类Z计算负梯度：$-g_Z(x_i)=Y_Z(x_i)-P_Z(x_i)$</li>
<li>训练一个模型（回归树）$h_A$ 来拟合负梯度 $-g_A(x_i)$</li>
<li>训练一个模型（回归树）$h_B$ 来拟合负梯度 $-g_B(x_i)$<br>$\cdots$</li>
<li>训练一个模型（回归树）$h_Z$ 来拟合负梯度 $-g_Z(x_i)$</li>
<li>$F_A:=F_A+\rho_Ah_A$</li>
<li>$F_B:=F_B+\rho_Bh_B$<br>$\cdots$</li>
<li>$F_Z:=F_Z+\rho_Zh_Z$</li>
</ul>
<h2 id="选择步长"><a href="#选择步长" class="headerlink" title="选择步长"></a>选择步长</h2><p>在预测过程中，我们每次添加一个弱分类器，但过多的分类器会导致过拟合，所以要对分类器的个数 $M$ 加以限制。另一方面，有人发现使用收缩技术可以很好地防止过拟合，一个简单的收缩技术就是在每一步中加入<strong>步长</strong>参数 $v$，即<br>$$F_m(\mathbf{x})=F_{m-1}(\mathbf{x})+v\cdot\rho_mh(\mathbf{x}),\quad 0&lt;v\le 1$$<br>我们现在有两个参数可以来约束模型：$M$ 和 $v$。它们之间互相影响，理想情况是从两个参数的联合分布中选出最合适的取值。不过，增大 $M$ 也会同时增大计算量。下图是在某个数据集上做的测试，四条曲线分别代表 $v\in\{1.0,0.25,0.125,0.06\}$。</p>
<p><img src="http://i.imgur.com/UfJc50o.png" alt=""></p>
<p>可以看出：</p>
<ol>
<li>当 $v$ 比较大时，可以看到很明显的过拟合：$M$ 增大到一定程度时模型达到最优解，再增大 $M$，模型会变差。</li>
<li>$v$ 值越小，收缩得越慢</li>
<li>$v$ 值越小，过拟合越不明显</li>
</ol>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf" target="_blank" rel="external">Gradient Boosting</a></li>
<li><a href="https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451" target="_blank" rel="external">Greedy Function Approximation: A Gradient Boosting Machine</a></li>
</ol>

      
    </div>

    <div>
      
        
      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/gbdt/" rel="tag">#gbdt</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/10/28/2016-10-28-Lars-Lasso-Stagewise/" rel="next" title="LARS与Lasso和Forward Stagewise">
                <i class="fa fa-chevron-left"></i> LARS与Lasso和Forward Stagewise
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/11/27/2016-11-27-Conjugate-Gradient-Method/" rel="prev" title="共轭梯度法（Conjugate Gradient Method）">
                共轭梯度法（Conjugate Gradient Method） <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/pic.PNG"
               alt="Wang Kx" />
          <p class="site-author-name" itemprop="name">Wang Kx</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">37</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">57</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        <div class="links-of-blogroll motion-element">
          
        </div>

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#什么是梯度提升-Gradient-Boosting"><span class="nav-number">1.</span> <span class="nav-text">什么是梯度提升(Gradient Boosting)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#一个小例子"><span class="nav-number">1.1.</span> <span class="nav-text">一个小例子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#与梯度下降的关系"><span class="nav-number">1.2.</span> <span class="nav-text">与梯度下降的关系</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度提升回归（Gradient-Boosting-for-Regression）"><span class="nav-number">2.</span> <span class="nav-text">梯度提升回归（Gradient Boosting for Regression）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#其他损失函数"><span class="nav-number">2.1.</span> <span class="nav-text">其他损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regression-with-Absolute-Loss"><span class="nav-number">2.2.</span> <span class="nav-text">Regression with Absolute Loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regression-with-Huber-Loss"><span class="nav-number">2.3.</span> <span class="nav-text">Regression with Huber Loss</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度提升分类（Gradient-Boosting-Classification）"><span class="nav-number">3.</span> <span class="nav-text">梯度提升分类（Gradient Boosting Classification）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#分类和回归的区别"><span class="nav-number">3.1.</span> <span class="nav-text">分类和回归的区别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#算法过程"><span class="nav-number">3.2.</span> <span class="nav-text">算法过程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#选择步长"><span class="nav-number">4.</span> <span class="nav-text">选择步长</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考资料"><span class="nav-number">5.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wang Kx</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>



      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  


  




<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  <script type="text/javascript" src="/lib/velocity/velocity.min.js"></script>

  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=0.5.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=0.5.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=0.5.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=0.5.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=0.5.0"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  



  



  
  
  

  

  

</body>
</html>
